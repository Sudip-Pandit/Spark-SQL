{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "174dd0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql.session import SparkSession\n",
    "spark = SparkSession.builder.appName('demo').master(\"local[*]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "e2366abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = spark.read.option(\"header\",True).csv(\"C:/Users/Sudip/Desktop/pyspark-project/2010-summary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "1b98e700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "d6ed515a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|    1|\n",
      "|    United States|            Ireland|  264|\n",
      "|    United States|              India|   69|\n",
      "|            Egypt|      United States|   24|\n",
      "|Equatorial Guinea|      United States|    1|\n",
      "|    United States|          Singapore|   25|\n",
      "|    United States|            Grenada|   54|\n",
      "|       Costa Rica|      United States|  477|\n",
      "|          Senegal|      United States|   29|\n",
      "|    United States|   Marshall Islands|   44|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "40420e6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count='1'),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Ireland', count='264'),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='India', count='69'),\n",
       " Row(DEST_COUNTRY_NAME='Egypt', ORIGIN_COUNTRY_NAME='United States', count='24'),\n",
       " Row(DEST_COUNTRY_NAME='Equatorial Guinea', ORIGIN_COUNTRY_NAME='United States', count='1'),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Singapore', count='25'),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Grenada', count='54'),\n",
       " Row(DEST_COUNTRY_NAME='Costa Rica', ORIGIN_COUNTRY_NAME='United States', count='477'),\n",
       " Row(DEST_COUNTRY_NAME='Senegal', ORIGIN_COUNTRY_NAME='United States', count='29'),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Marshall Islands', count='44')]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "e5535894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Sort [count#980 ASC NULLS FIRST], true, 0\n",
      "+- Exchange rangepartitioning(count#980 ASC NULLS FIRST, 200), ENSURE_REQUIREMENTS, [id=#1153]\n",
      "   +- FileScan csv [DEST_COUNTRY_NAME#978,ORIGIN_COUNTRY_NAME#979,count#980] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/C:/Users/Sudip/Desktop/pyspark-project/2010-summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string,ORIGIN_COUNTRY_NAME:string,count:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#we know the physcal plan by using this command\n",
    "# We can apply sort function as a transformation to the previous dataframe\n",
    "# \n",
    "df1.sort(\"count\").explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "5c95f1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can make any dataframe to table or view by using this command\n",
    "df1.createOrReplaceTempView(\"flightData\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "a7e92d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql1 = spark.sql(\"select dest_country_name, count, origin_country_name from flightData\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "bc309227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----+-------------------+\n",
      "|dest_country_name|count|origin_country_name|\n",
      "+-----------------+-----+-------------------+\n",
      "|    United States|    1|            Romania|\n",
      "|    United States|  264|            Ireland|\n",
      "|    United States|   69|              India|\n",
      "|            Egypt|   24|      United States|\n",
      "|Equatorial Guinea|    1|      United States|\n",
      "+-----------------+-----+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql1.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "d83dbe72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|dest_country_name|\n",
      "+-----------------+\n",
      "|            Egypt|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now we are good to use our sql skills while doing transformations as per our business logic\n",
    "spark.sql(\"select dest_country_name from flightData where dest_country_name='Egypt'\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "9036ecdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+\n",
      "|count(dest_country_name)|\n",
      "+------------------------+\n",
      "|                     255|\n",
      "+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select count(dest_country_name) from flightData\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "ac6eab23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------+\n",
      "|dest_country_name|count(1)|\n",
      "+-----------------+--------+\n",
      "|         Anguilla|       1|\n",
      "|           Russia|       1|\n",
      "|         Paraguay|       1|\n",
      "|          Senegal|       1|\n",
      "|           Sweden|       1|\n",
      "+-----------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select dest_country_name, count(1) from flightData Group By dest_country_name\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "b83581bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|    1|\n",
      "|    United States|            Ireland|  264|\n",
      "|    United States|              India|   69|\n",
      "|            Egypt|      United States|   24|\n",
      "|Equatorial Guinea|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from flightData\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "4d2723df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----+\n",
      "|dest_country_name|rank|\n",
      "+-----------------+----+\n",
      "|         Anguilla|   1|\n",
      "|         Paraguay|   1|\n",
      "|           Russia|   1|\n",
      "|          Senegal|   1|\n",
      "|           Sweden|   1|\n",
      "+-----------------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select dest_country_name, rank() over(partition by dest_country_name order by count) rank \\\n",
    "from flightData\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "e553ac06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+----------+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|Row_number|\n",
      "+--------------------+-------------------+-----+----------+\n",
      "|       United States|            Romania|    1|         1|\n",
      "|   Equatorial Guinea|      United States|    1|         2|\n",
      "|               Malta|      United States|    1|         3|\n",
      "|Saint Vincent and...|      United States|    1|         4|\n",
      "|            Slovakia|      United States|    1|         5|\n",
      "+--------------------+-------------------+-----+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select *, \\\n",
    "    row_number() over(order by count) Row_number \\\n",
    "    from flightData\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "b62edcde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+----------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|Dense_Rank|\n",
      "+-----------------+-------------------+-----+----------+\n",
      "|    United States|           Anguilla|   20|         1|\n",
      "|    United States|             Russia|  156|         1|\n",
      "|    United States|            Senegal|   46|         1|\n",
      "|    United States|             Sweden|   73|         1|\n",
      "|    United States|           Kiribati|   18|         1|\n",
      "+-----------------+-------------------+-----+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select *, \\\n",
    "    dense_rank() over(partition by origin_country_name order by count) Dense_Rank \\\n",
    "    from flightData\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "b5e96864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|max(count)|\n",
      "+----------+\n",
      "|        21|\n",
      "|        90|\n",
      "|       152|\n",
      "|        29|\n",
      "|        65|\n",
      "+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    " #in Python in oder to query aggregate function we need to import\n",
    "from pyspark.sql.functions import max\n",
    "spark.sql(\"select max(count) from flightData group by dest_country_name\").show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "ac3a9071",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import max, min, count, avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "dd027156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|min(count)|\n",
      "+----------+\n",
      "|         1|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select min(count) from flightData\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "5cde7c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|max(count)|\n",
      "+----------+\n",
      "|       995|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select max(count) from flightData\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "1bef7e02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+\n",
      "|avg(CAST(count AS DOUBLE))|\n",
      "+--------------------------+\n",
      "|         1655.956862745098|\n",
      "+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select avg(count) from flightData\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "a043fc9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------+\n",
      "|round(avg(CAST(count AS DOUBLE)), 0)|\n",
      "+------------------------------------+\n",
      "|                              1656.0|\n",
      "+------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select round(avg(count)) from flightData\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "40ba9214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----------+\n",
      "|DEST_COUNTRY_NAME|dest_total|\n",
      "+-----------------+----------+\n",
      "|    United States|  384932.0|\n",
      "|           Canada|    8271.0|\n",
      "|           Mexico|    6200.0|\n",
      "|   United Kingdom|    1629.0|\n",
      "|          Germany|    1392.0|\n",
      "+-----------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT DEST_COUNTRY_NAME, sum(count) as dest_total\n",
    "FROM flightData\n",
    "GROUP BY DEST_COUNTRY_NAME\n",
    "ORDER BY sum(count) DESC\n",
    "LIMIT 5\n",
    "\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a615ed",
   "metadata": {},
   "source": [
    "### Working with pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "a57b9a09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|dest_country_name|\n",
      "+-----------------+\n",
      "|    United States|\n",
      "|    United States|\n",
      "|    United States|\n",
      "|            Egypt|\n",
      "|Equatorial Guinea|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#select only one column in pyspark\n",
    "\n",
    "df1.select(df1[\"dest_country_name\"]).limit(5).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d99653",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"csv\").option(\"header\", True) \\\n",
    "    .option(\"inferSchema\", True) \\\n",
    "    .load(\"C:/Users/Sudip/Desktop/pyspark-project/*.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
