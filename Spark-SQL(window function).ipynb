{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "257a0749",
   "metadata": {},
   "source": [
    "### Window SQL in Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "056c7610",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d283ffce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.session import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed388a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('Nth highestSalary').master(\"local[*]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d710f467",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"id\", \"salary\"]\n",
    "\n",
    "data= [(1,2000),\n",
    "      (2,3000),\n",
    "      (3,4000),\n",
    "      (4,3000),\n",
    "      (5,6000),\n",
    "      (6,2000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "037605ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: bigint, salary: bigint]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = spark.createDataFrame(data,cols)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f687a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|salary|\n",
      "+---+------+\n",
      "|  1|  2000|\n",
      "|  2|  3000|\n",
      "|  3|  4000|\n",
      "|  4|  3000|\n",
      "|  5|  6000|\n",
      "|  6|  2000|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39586f2",
   "metadata": {},
   "source": [
    "### row_number Window Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a016b84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10d2fb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "windowSpec  = Window.partitionBy(\"id\").orderBy(\"salary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "43322f19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------+\n",
      "|id |salary|row_number|\n",
      "+---+------+----------+\n",
      "|6  |2000  |1         |\n",
      "|5  |6000  |1         |\n",
      "|1  |2000  |1         |\n",
      "|3  |4000  |1         |\n",
      "|2  |3000  |1         |\n",
      "|4  |3000  |1         |\n",
      "+---+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"row_number\",row_number().over(windowSpec)) \\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5323588b",
   "metadata": {},
   "source": [
    "### Rank Window Function\n",
    "rank() window function is used to provide a rank to the result within a window partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3340d228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----+\n",
      "| id|salary|rank|\n",
      "+---+------+----+\n",
      "|  6|  2000|   1|\n",
      "|  5|  6000|   1|\n",
      "|  1|  2000|   1|\n",
      "|  3|  4000|   1|\n",
      "|  2|  3000|   1|\n",
      "|  4|  3000|   1|\n",
      "+---+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import rank\n",
    "df.withColumn(\"rank\",rank().over(windowSpec)) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d48a3c",
   "metadata": {},
   "source": [
    "### dense_rank window function\n",
    "dense_rank() window function is used to get the result with rank of rows within a window partition without any gaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a768914b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------+\n",
      "| id|salary|dense_rank|\n",
      "+---+------+----------+\n",
      "|  6|  2000|         1|\n",
      "|  5|  6000|         1|\n",
      "|  1|  2000|         1|\n",
      "|  3|  4000|         1|\n",
      "|  2|  3000|         1|\n",
      "|  4|  3000|         1|\n",
      "+---+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import dense_rank\n",
    "df.withColumn(\"dense_rank\",dense_rank().over(windowSpec)) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947b3d5b",
   "metadata": {},
   "source": [
    "### percent_rank Window Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "21bc5fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------------+\n",
      "| id|salary|percent_rank|\n",
      "+---+------+------------+\n",
      "|  6|  2000|         0.0|\n",
      "|  5|  6000|         0.0|\n",
      "|  1|  2000|         0.0|\n",
      "|  3|  4000|         0.0|\n",
      "|  2|  3000|         0.0|\n",
      "|  4|  3000|         0.0|\n",
      "+---+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import percent_rank\n",
    "df.withColumn(\"percent_rank\",percent_rank().over(windowSpec)) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfbe5b8",
   "metadata": {},
   "source": [
    "### ntile Window Function\n",
    "ntile() window function returns the relative rank of result rows within a window partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f494cafb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+\n",
      "| id|salary|ntile|\n",
      "+---+------+-----+\n",
      "|  6|  2000|    1|\n",
      "|  5|  6000|    1|\n",
      "|  1|  2000|    1|\n",
      "|  3|  4000|    1|\n",
      "|  2|  3000|    1|\n",
      "|  4|  3000|    1|\n",
      "+---+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import ntile\n",
    "df.withColumn(\"ntile\",ntile(2).over(windowSpec)) \\\n",
    "    .show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b0ae423e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- emp_name: string (nullable = true)\n",
      " |-- dep: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n",
      "+--------+---------+------+\n",
      "|emp_name|dep      |salary|\n",
      "+--------+---------+------+\n",
      "|James   |Sales    |3000  |\n",
      "|Michael |Sales    |4600  |\n",
      "|Robert  |Sales    |4100  |\n",
      "|Maria   |Finance  |3000  |\n",
      "|James   |Sales    |3000  |\n",
      "|Scott   |Finance  |3300  |\n",
      "|Jen     |Finance  |3900  |\n",
      "|Jeff    |Marketing|3000  |\n",
      "|Kumar   |Marketing|2000  |\n",
      "|Saif    |Sales    |4100  |\n",
      "+--------+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### PySpark Window Aggregate Functions\n",
    "data1 = ((\"James\", \"Sales\", 3000), \\\n",
    "    (\"Michael\", \"Sales\", 4600),  \\\n",
    "    (\"Robert\", \"Sales\", 4100),   \\\n",
    "    (\"Maria\", \"Finance\", 3000),  \\\n",
    "    (\"James\", \"Sales\", 3000),    \\\n",
    "    (\"Scott\", \"Finance\", 3300),  \\\n",
    "    (\"Jen\", \"Finance\", 3900),    \\\n",
    "    (\"Jeff\", \"Marketing\", 3000), \\\n",
    "    (\"Kumar\", \"Marketing\", 2000),\\\n",
    "    (\"Saif\", \"Sales\", 4100) \\\n",
    "  )\n",
    " \n",
    "columns= [\"emp_name\", \"dep\", \"salary\"]\n",
    "df1 = spark.createDataFrame(data =data1, schema = columns)\n",
    "df1.printSchema()\n",
    "df1.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6d38a1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "windowSpecAgg  = Window.partitionBy(\"dep\")\n",
    "from pyspark.sql.functions import col,avg,sum,min,max,row_number "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dd7ac1a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+------+------+\n",
      "|emp_name|      dep|salary|   avg|\n",
      "+--------+---------+------+------+\n",
      "|   James|    Sales|  3000|3760.0|\n",
      "| Michael|    Sales|  4600|3760.0|\n",
      "|  Robert|    Sales|  4100|3760.0|\n",
      "|   James|    Sales|  3000|3760.0|\n",
      "|    Saif|    Sales|  4100|3760.0|\n",
      "|   Maria|  Finance|  3000|3400.0|\n",
      "|   Scott|  Finance|  3300|3400.0|\n",
      "|     Jen|  Finance|  3900|3400.0|\n",
      "|    Jeff|Marketing|  3000|2500.0|\n",
      "|   Kumar|Marketing|  2000|2500.0|\n",
      "+--------+---------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.withColumn(\"avg\", avg(col(\"salary\")).over(windowSpecAgg)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ddd2cf7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+------+-----+\n",
      "|emp_name|      dep|salary|  sum|\n",
      "+--------+---------+------+-----+\n",
      "|   James|    Sales|  3000|18800|\n",
      "| Michael|    Sales|  4600|18800|\n",
      "|  Robert|    Sales|  4100|18800|\n",
      "|   James|    Sales|  3000|18800|\n",
      "|    Saif|    Sales|  4100|18800|\n",
      "|   Maria|  Finance|  3000|10200|\n",
      "|   Scott|  Finance|  3300|10200|\n",
      "|     Jen|  Finance|  3900|10200|\n",
      "|    Jeff|Marketing|  3000| 5000|\n",
      "|   Kumar|Marketing|  2000| 5000|\n",
      "+--------+---------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.withColumn(\"sum\", sum(col(\"salary\")).over(windowSpecAgg)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a9ffef17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+------+----+\n",
      "|emp_name|      dep|salary| min|\n",
      "+--------+---------+------+----+\n",
      "|   James|    Sales|  3000|3000|\n",
      "| Michael|    Sales|  4600|3000|\n",
      "|  Robert|    Sales|  4100|3000|\n",
      "|   James|    Sales|  3000|3000|\n",
      "|    Saif|    Sales|  4100|3000|\n",
      "|   Maria|  Finance|  3000|3000|\n",
      "|   Scott|  Finance|  3300|3000|\n",
      "|     Jen|  Finance|  3900|3000|\n",
      "|    Jeff|Marketing|  3000|2000|\n",
      "|   Kumar|Marketing|  2000|2000|\n",
      "+--------+---------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.withColumn(\"min\", min(col(\"salary\")).over(windowSpecAgg)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ff9d08c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+------+----+\n",
      "|emp_name|      dep|salary| max|\n",
      "+--------+---------+------+----+\n",
      "|   James|    Sales|  3000|4600|\n",
      "| Michael|    Sales|  4600|4600|\n",
      "|  Robert|    Sales|  4100|4600|\n",
      "|   James|    Sales|  3000|4600|\n",
      "|    Saif|    Sales|  4100|4600|\n",
      "|   Maria|  Finance|  3000|3900|\n",
      "|   Scott|  Finance|  3300|3900|\n",
      "|     Jen|  Finance|  3900|3900|\n",
      "|    Jeff|Marketing|  3000|3000|\n",
      "|   Kumar|Marketing|  2000|3000|\n",
      "+--------+---------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.withColumn(\"max\", max(col(\"salary\")).over(windowSpecAgg)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb57e59",
   "metadata": {},
   "source": [
    "### cume_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "122cf235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---------+\n",
      "| id|salary|cume_dist|\n",
      "+---+------+---------+\n",
      "|  6|  2000|      1.0|\n",
      "|  5|  6000|      1.0|\n",
      "|  1|  2000|      1.0|\n",
      "|  3|  4000|      1.0|\n",
      "|  2|  3000|      1.0|\n",
      "|  4|  3000|      1.0|\n",
      "+---+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import cume_dist\n",
    "df.withColumn(\"cume_dist\",cume_dist().over(windowSpec)) \\\n",
    "   .show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7344598a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----+\n",
      "| id|salary| lag|\n",
      "+---+------+----+\n",
      "|  6|  2000|null|\n",
      "|  5|  6000|null|\n",
      "|  1|  2000|null|\n",
      "|  3|  4000|null|\n",
      "|  2|  3000|null|\n",
      "|  4|  3000|null|\n",
      "+---+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lag    \n",
    "df.withColumn(\"lag\",lag(\"salary\",2).over(windowSpec)) \\\n",
    "      .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3b1c85dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----+\n",
      "| id|salary|lead|\n",
      "+---+------+----+\n",
      "|  6|  2000|null|\n",
      "|  5|  6000|null|\n",
      "|  1|  2000|null|\n",
      "|  3|  4000|null|\n",
      "|  2|  3000|null|\n",
      "|  4|  3000|null|\n",
      "+---+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lead    \n",
    "df.withColumn(\"lead\",lead(\"salary\",2).over(windowSpec)) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4703216",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
