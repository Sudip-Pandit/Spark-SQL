{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a338ba1",
   "metadata": {},
   "source": [
    "### Window SQL in Spark\n",
    "We need to understand the window function in PySpark sql. It is also called as statistical operations such as rank, row number etc. It is mostly asked interview question in Big Data Engineering interview. Now, PySpark window function is growing to perform different transformations operations\n",
    "Mainly there are three types of window function in PySpark\n",
    "1) Analytical Window Function\n",
    "\n",
    "2) Rank Window Function\n",
    "\n",
    "3) Aggregate Window FUnction\n",
    "\n",
    "To perform window function operation on a group of rows first, we need to first partition i.e. define the group of data rows using window.partition() function,\n",
    "\n",
    "=> and for row number and rank function we need to additionally order by on partition data using ORDER BY clause.\n",
    "\n",
    "\n",
    "1) Analytical functions\n",
    "=>>> An analytic function is a function that returns a result after operating on data or a finite set of rows partitioned by a SELECT clause or in the ORDER BY clause. It returns a result in the same number of rows as the number of input rows. E.g. lead(), lag(), cume_dist().\n",
    "\n",
    "2) Ranking Function\n",
    "=>>>The function returns the statistical rank of a given value for each row in a partition or group. The goal of this function is to provide consecutive numbering of the rows in the resultant column, set by the order selected in the Window.partition for each partition specified in the OVER clause. E.g. row_number(), rank(), dense_rank(), etc.\n",
    "\n",
    "3) Aggregate function\n",
    "=>>> An aggregate function or aggregation function is a function where the values of multiple rows are grouped to form a single summary value. The definition of the groups of rows on which they operate is done by using the SQL GROUP BY clause. E.g. AVERAGE, SUM, MIN, MAX, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7aefbd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf3cb7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.session import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2064a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('Nth highestSalary').master(\"local[*]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d399b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"id\", \"salary\"]\n",
    "\n",
    "data= [(1,2000),\n",
    "      (2,3000),\n",
    "      (3,4000),\n",
    "      (4,3000),\n",
    "      (5,6000),\n",
    "      (6,2000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88982ffe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: bigint, salary: bigint]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = spark.createDataFrame(data,cols)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76288508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|salary|\n",
      "+---+------+\n",
      "|  1|  2000|\n",
      "|  2|  3000|\n",
      "|  3|  4000|\n",
      "|  4|  3000|\n",
      "|  5|  6000|\n",
      "|  6|  2000|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271bba8a",
   "metadata": {},
   "source": [
    "### row_number Window Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "97ea0983",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "93bd03ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "windowSpec  = Window.partitionBy(\"id\").orderBy(\"salary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45aa4d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------+\n",
      "|id |salary|row_number|\n",
      "+---+------+----------+\n",
      "|6  |2000  |1         |\n",
      "|5  |6000  |1         |\n",
      "|1  |2000  |1         |\n",
      "|3  |4000  |1         |\n",
      "|2  |3000  |1         |\n",
      "|4  |3000  |1         |\n",
      "+---+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"row_number\",row_number().over(windowSpec)) \\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37652a84",
   "metadata": {},
   "source": [
    "### Rank Window Function\n",
    "rank() window function is used to provide a rank to the result within a window partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "79ad798e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----+\n",
      "| id|salary|rank|\n",
      "+---+------+----+\n",
      "|  6|  2000|   1|\n",
      "|  5|  6000|   1|\n",
      "|  1|  2000|   1|\n",
      "|  3|  4000|   1|\n",
      "|  2|  3000|   1|\n",
      "|  4|  3000|   1|\n",
      "+---+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import rank\n",
    "df.withColumn(\"rank\",rank().over(windowSpec)) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a3f91b",
   "metadata": {},
   "source": [
    "### dense_rank window function\n",
    "dense_rank() window function is used to get the result with rank of rows within a window partition without any gaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "954516ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------+\n",
      "| id|salary|dense_rank|\n",
      "+---+------+----------+\n",
      "|  6|  2000|         1|\n",
      "|  5|  6000|         1|\n",
      "|  1|  2000|         1|\n",
      "|  3|  4000|         1|\n",
      "|  2|  3000|         1|\n",
      "|  4|  3000|         1|\n",
      "+---+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import dense_rank\n",
    "df.withColumn(\"dense_rank\",dense_rank().over(windowSpec)) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01e8feb",
   "metadata": {},
   "source": [
    "### percent_rank Window Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fa0e1214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------------+\n",
      "| id|salary|percent_rank|\n",
      "+---+------+------------+\n",
      "|  6|  2000|         0.0|\n",
      "|  5|  6000|         0.0|\n",
      "|  1|  2000|         0.0|\n",
      "|  3|  4000|         0.0|\n",
      "|  2|  3000|         0.0|\n",
      "|  4|  3000|         0.0|\n",
      "+---+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import percent_rank\n",
    "df.withColumn(\"percent_rank\",percent_rank().over(windowSpec)) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13011d31",
   "metadata": {},
   "source": [
    "### ntile Window Function\n",
    "ntile() window function returns the relative rank of result rows within a window partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e1da9533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+\n",
      "| id|salary|ntile|\n",
      "+---+------+-----+\n",
      "|  6|  2000|    1|\n",
      "|  5|  6000|    1|\n",
      "|  1|  2000|    1|\n",
      "|  3|  4000|    1|\n",
      "|  2|  3000|    1|\n",
      "|  4|  3000|    1|\n",
      "+---+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import ntile\n",
    "df.withColumn(\"ntile\",ntile(2).over(windowSpec)) \\\n",
    "    .show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08c192e",
   "metadata": {},
   "source": [
    "### Aggregate function\n",
    "An aggregate function or aggregation function is a function where the values of multiple rows are grouped to form a single summary value. \n",
    "The definition of the groups of rows on which they operate is done by using the SQL GROUP BY clause. E.g. AVERAGE, SUM, MIN, MAX, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e2ce62c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- emp_name: string (nullable = true)\n",
      " |-- dep: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n",
      "+--------+---------+------+\n",
      "|emp_name|dep      |salary|\n",
      "+--------+---------+------+\n",
      "|James   |Sales    |3000  |\n",
      "|Michael |Sales    |4600  |\n",
      "|Robert  |Sales    |4100  |\n",
      "|Maria   |Finance  |3000  |\n",
      "|James   |Sales    |3000  |\n",
      "|Scott   |Finance  |3300  |\n",
      "|Jen     |Finance  |3900  |\n",
      "|Jeff    |Marketing|3000  |\n",
      "|Kumar   |Marketing|2000  |\n",
      "|Saif    |Sales    |4100  |\n",
      "+--------+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### PySpark Window Aggregate Functions\n",
    "data1 = ((\"James\", \"Sales\", 3000), \\\n",
    "    (\"Michael\", \"Sales\", 4600),  \\\n",
    "    (\"Robert\", \"Sales\", 4100),   \\\n",
    "    (\"Maria\", \"Finance\", 3000),  \\\n",
    "    (\"James\", \"Sales\", 3000),    \\\n",
    "    (\"Scott\", \"Finance\", 3300),  \\\n",
    "    (\"Jen\", \"Finance\", 3900),    \\\n",
    "    (\"Jeff\", \"Marketing\", 3000), \\\n",
    "    (\"Kumar\", \"Marketing\", 2000),\\\n",
    "    (\"Saif\", \"Sales\", 4100) \\\n",
    "  )\n",
    " \n",
    "columns= [\"emp_name\", \"dep\", \"salary\"]\n",
    "df1 = spark.createDataFrame(data =data1, schema = columns)\n",
    "df1.printSchema()\n",
    "df1.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c10a4f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "windowSpecAgg  = Window.partitionBy(\"dep\")\n",
    "from pyspark.sql.functions import col,avg,sum,min,max,row_number "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8c67aad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+------+------+\n",
      "|emp_name|      dep|salary|   avg|\n",
      "+--------+---------+------+------+\n",
      "|   James|    Sales|  3000|3760.0|\n",
      "| Michael|    Sales|  4600|3760.0|\n",
      "|  Robert|    Sales|  4100|3760.0|\n",
      "|   James|    Sales|  3000|3760.0|\n",
      "|    Saif|    Sales|  4100|3760.0|\n",
      "|   Maria|  Finance|  3000|3400.0|\n",
      "|   Scott|  Finance|  3300|3400.0|\n",
      "|     Jen|  Finance|  3900|3400.0|\n",
      "|    Jeff|Marketing|  3000|2500.0|\n",
      "|   Kumar|Marketing|  2000|2500.0|\n",
      "+--------+---------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.withColumn(\"avg\", avg(col(\"salary\")).over(windowSpecAgg)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ccde02bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+------+-----+\n",
      "|emp_name|      dep|salary|  sum|\n",
      "+--------+---------+------+-----+\n",
      "|   James|    Sales|  3000|18800|\n",
      "| Michael|    Sales|  4600|18800|\n",
      "|  Robert|    Sales|  4100|18800|\n",
      "|   James|    Sales|  3000|18800|\n",
      "|    Saif|    Sales|  4100|18800|\n",
      "|   Maria|  Finance|  3000|10200|\n",
      "|   Scott|  Finance|  3300|10200|\n",
      "|     Jen|  Finance|  3900|10200|\n",
      "|    Jeff|Marketing|  3000| 5000|\n",
      "|   Kumar|Marketing|  2000| 5000|\n",
      "+--------+---------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.withColumn(\"sum\", sum(col(\"salary\")).over(windowSpecAgg)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6b857a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+------+----+\n",
      "|emp_name|      dep|salary| min|\n",
      "+--------+---------+------+----+\n",
      "|   James|    Sales|  3000|3000|\n",
      "| Michael|    Sales|  4600|3000|\n",
      "|  Robert|    Sales|  4100|3000|\n",
      "|   James|    Sales|  3000|3000|\n",
      "|    Saif|    Sales|  4100|3000|\n",
      "|   Maria|  Finance|  3000|3000|\n",
      "|   Scott|  Finance|  3300|3000|\n",
      "|     Jen|  Finance|  3900|3000|\n",
      "|    Jeff|Marketing|  3000|2000|\n",
      "|   Kumar|Marketing|  2000|2000|\n",
      "+--------+---------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.withColumn(\"min\", min(col(\"salary\")).over(windowSpecAgg)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f9fc0bdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+------+----+\n",
      "|emp_name|      dep|salary| max|\n",
      "+--------+---------+------+----+\n",
      "|   James|    Sales|  3000|4600|\n",
      "| Michael|    Sales|  4600|4600|\n",
      "|  Robert|    Sales|  4100|4600|\n",
      "|   James|    Sales|  3000|4600|\n",
      "|    Saif|    Sales|  4100|4600|\n",
      "|   Maria|  Finance|  3000|3900|\n",
      "|   Scott|  Finance|  3300|3900|\n",
      "|     Jen|  Finance|  3900|3900|\n",
      "|    Jeff|Marketing|  3000|3000|\n",
      "|   Kumar|Marketing|  2000|3000|\n",
      "+--------+---------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.withColumn(\"max\", max(col(\"salary\")).over(windowSpecAgg)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116e45c9",
   "metadata": {},
   "source": [
    "### cume_dist\n",
    "cume_dist() window function is used to get the cumulative distribution within a window partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0ef59686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---------+\n",
      "| id|salary|cume_dist|\n",
      "+---+------+---------+\n",
      "|  6|  2000|      1.0|\n",
      "|  5|  6000|      1.0|\n",
      "|  1|  2000|      1.0|\n",
      "|  3|  4000|      1.0|\n",
      "|  2|  3000|      1.0|\n",
      "|  4|  3000|      1.0|\n",
      "+---+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import cume_dist\n",
    "df.withColumn(\"cume_dist\",cume_dist().over(windowSpec)) \\\n",
    "   .show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe2c5ee",
   "metadata": {},
   "source": [
    "### Using lag()\n",
    "A lag() function is used to access previous rows’ data as per the defined offset value in the function.\n",
    "This function is similar to the LAG in SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cdefba44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----+\n",
      "| id|salary| lag|\n",
      "+---+------+----+\n",
      "|  6|  2000|null|\n",
      "|  5|  6000|null|\n",
      "|  1|  2000|null|\n",
      "|  3|  4000|null|\n",
      "|  2|  3000|null|\n",
      "|  4|  3000|null|\n",
      "+---+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lag    \n",
    "df.withColumn(\"lag\",lag(\"salary\",2).over(windowSpec)) \\\n",
    "      .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6affb5e",
   "metadata": {},
   "source": [
    "###  Using lead()\n",
    "A lead() function is used to access next rows data as per the defined offset value in the function.\n",
    "This function is similar to the LEAD in SQL and just opposite to lag() function or LAG in SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b4cdf44d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----+\n",
      "| id|salary|lead|\n",
      "+---+------+----+\n",
      "|  6|  2000|null|\n",
      "|  5|  6000|null|\n",
      "|  1|  2000|null|\n",
      "|  3|  4000|null|\n",
      "|  2|  3000|null|\n",
      "|  4|  3000|null|\n",
      "+---+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lead    \n",
    "df.withColumn(\"lead\",lead(\"salary\",2).over(windowSpec)) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e2da00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
